{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiments towards COVID-19 Vaccine in South Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "What were we trying to achieve? What are the key results/insights did we obtain? How did we get to these insights? What data was used and how was it processed? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-088be4cc02be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#nltk.download(Stopwords)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#Embedding dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Importing useful packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from english_words import english_words_set\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#nltk.download(Stopwords)\n",
    "#Embedding dependencies\n",
    "from gensim.models import word2vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imports\n",
    "df1 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'QnGqQx-EA4g')\n",
    "df2 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = '0jejyuRQLxc')\n",
    "df3 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'gklQh5v0UuI')\n",
    "df4 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'mcPzF-GSoiY')\n",
    "df5 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'Q66BMCJMeHY')\n",
    "df6 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = '-HLZwv_Eh7w')\n",
    "df7 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'VwZFkMlUQGs')\n",
    "df8 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'fCgbUyxXhAM')\n",
    "df9 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'Fs1bel0HM6Y')\n",
    "df10 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = '09PASKB3sgU')\n",
    "df11 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'HfN1oPBGXRk')\n",
    "df12 = pd.read_excel('Covid_vaccine_sentiments.xlsx',sheet_name = 'E3lSVH64u9k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2757"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape[0] + df2.shape[0] + df3.shape[0] + df4.shape[0] + df5.shape[0] + df6.shape[0] + df7.shape[0] + df8.shape[0] + df9.shape[0] + df10.shape[0] + df11.shape[0] + df12.shape[0]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining into one main data frame. # Comments without replies from all videos\n",
    "frames_comments = [df1[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df2[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df3[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df4[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df5[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df6[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']], \n",
    "                        df7[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df8[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df9[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df10[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                        df11[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']],\n",
    "                       df12[['Name', 'Comment', 'Time', 'Likes', 'Reply Count']]]\n",
    "comments  = pd.concat(frames_comments).dropna().reset_index() \n",
    "comments['Comment'] = comments['Comment'].astype('str') # converting comments to strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining into one main data frame. # Comments without replies from all videos\n",
    "frames_replies = [df1[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df2[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df3[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df4[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df5[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df6[['Reply Author','Reply', 'Published', 'Updated']], \n",
    "                        df7[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df8[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df9[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df10[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df11[['Reply Author','Reply', 'Published', 'Updated']],\n",
    "                        df12[['Reply Author','Reply', 'Published', 'Updated']]]\n",
    "replies_to_comments = pd.concat(frames_replies).dropna().reset_index()\n",
    "replies_to_comments['Reply'] = replies_to_comments['Reply'].astype('str') # Converting to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replies_to_comments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing url\n",
    "replies_cleaned = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', replies_to_comments['Reply'].iloc[i], flags=re.MULTILINE) for i in range(len(replies_to_comments))]\n",
    "comments_cleaned = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', comments['Comment'].iloc[i], flags=re.MULTILINE) for i in range(len(comments))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing html\n",
    "replies_cleaned = [re.sub(r'<[^<]+?>', '', replies_cleaned[i], flags=re.MULTILINE) for i in range(len(replies_cleaned))]\n",
    "comments_cleaned = [re.sub(r'<[^<]+?>', '', comments_cleaned[i], flags=re.MULTILINE) for i in range(len(comments_cleaned))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters\n",
    "replies_cleaned = [re.sub(r'[^A-Za-z0-9]+', ' ', replies_cleaned[i], flags=re.MULTILINE) for i in range(len(replies_cleaned))]\n",
    "comments_cleaned = [re.sub(r'[^A-Za-z0-9]+', ' ', comments_cleaned[i], flags=re.MULTILINE) for i in range(len(comments_cleaned))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "reply_tokens = [re.findall(\"\\w+\", replies_cleaned[i].lower()) for i in range(len(replies_cleaned))]\n",
    "comment_tokens = [re.findall(\"\\w+\", comments_cleaned[i].lower()) for i in range(len(comments_cleaned))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning proper english words only\n",
    "def going_english(sentences):\n",
    "    \"\"\"Input\n",
    "       sentences: A list containing lists of tokenized sentences\n",
    "       Output\n",
    "       english_tokens: A list containing lists of tokenized sentences with english words only\n",
    "    \"\"\"\n",
    "    english_tokens = []\n",
    "    for i in range(len(sentences)):\n",
    "        eng_wrds = []\n",
    "        for word in sentences[i]:\n",
    "            if word in english_words_set: # comparing with the imported english words\n",
    "                eng_wrds.append(word)\n",
    "        english_tokens.append(eng_wrds)\n",
    "    return english_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reply_tokens = going_english(reply_tokens)\n",
    "english_comment_tokens = going_english(comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english_comment_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning proper english words only\n",
    "def remove_stopwords(sentences):\n",
    "    \"\"\"Input\n",
    "       sentences: A list containing lists of tokenized sentences\n",
    "       Output\n",
    "       english_tokens: A list containing lists of tokenized sentences without english stopwords only\n",
    "    \"\"\"\n",
    "    stop_words = get_stop_words('english')\n",
    "    no_stopwords = []\n",
    "    for i in range(len(sentences)):\n",
    "        not_stopword = []\n",
    "        for word in sentences[i]:\n",
    "            if word not in stop_words: # comparing with the imported list of stopwords\n",
    "                not_stopword.append(word)\n",
    "        no_stopwords.append(not_stopword)\n",
    "    return no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reply_tokens = remove_stopwords(english_reply_tokens)\n",
    "cleaned_comment_tokens = remove_stopwords(english_comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cleaned_reply_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra steps for data saving and and retrival because of package availability issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am missing some preprocessing packages on the kernel that has packages for training word embedding.\n",
    "# so I first preprocess using another kernel, store the preprocessed tokenized word into data frames.\n",
    "# I then switch to the kernel on which I'll be training the word embeddings . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_replies = pd.DataFrame(cleaned_reply_tokens)\n",
    "df_comments = pd.DataFrame(cleaned_comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_replies.to_csv('replies_dataframe.csv')\n",
    "df_comments.to_csv('comments_dataframe.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Switching kernels Code for embedding traing starst here. \n",
    "# re reading the data frames \n",
    "df_replies = \n",
    "df_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stupid', 'best']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving the tokenized sentences for embedding training\n",
    "\n",
    "comments_retrieved = [list(tr.iloc[i].dropna()) for i in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# UNIT TEST COMMENT: Candidate for Table Driven Tests\n",
    "# GRADED FUNCTION: get_count\n",
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # fill this with word counts\n",
    "    ### START CODE HERE \n",
    "    word_count_dict.update(Counter(word_l))\n",
    "            \n",
    "    ### END CODE HERE ### \n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Going over all sentences to get word counts and create word frequencies\n",
    "# Depends on get_count function\n",
    "def corpus_word_count(sentences):\n",
    "    full_count_comments = {}\n",
    "    for i in range(len(sentences)):\n",
    "        full_count_comments.update(get_count(sentences[i]))\n",
    "    return full_count_comments\n",
    "#full_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies_word_counts = corpus_word_count(cleaned_reply_tokens)\n",
    "comments_word_counts = corpus_word_count(cleaned_comment_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocub_replies = replies_word_counts.keys()\n",
    "Vocub_comments = comments_word_counts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocub_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Arranging in a descending order\n",
    "comment_word_counts_sorted = sorted(full_count_comments.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('politician', 5),\n",
       " ('pcr', 5),\n",
       " ('annoy', 5),\n",
       " ('quot', 4),\n",
       " ('group', 4),\n",
       " ('7', 4),\n",
       " ('feet', 4),\n",
       " ('attend', 4),\n",
       " ('hoof', 4),\n",
       " ('chew', 4),\n",
       " ('cud', 4),\n",
       " ('know', 3),\n",
       " ('39', 3),\n",
       " ('would', 3),\n",
       " ('dr', 3),\n",
       " ('bacteria', 3),\n",
       " ('my', 3),\n",
       " ('spread', 3),\n",
       " ('cell', 3),\n",
       " ('sa', 3),\n",
       " ('rate', 3),\n",
       " ('remedi', 3),\n",
       " ('homeless', 3),\n",
       " ('pass', 3),\n",
       " ('eat', 3),\n",
       " ('men', 3),\n",
       " ('plagu', 3),\n",
       " ('figur', 3),\n",
       " ('school', 3),\n",
       " ('tribul', 3),\n",
       " ('percept', 3),\n",
       " ('socialist', 3),\n",
       " ('0', 3),\n",
       " ('da', 3),\n",
       " ('curs', 3),\n",
       " ('among', 3),\n",
       " ('diagnos', 3),\n",
       " ('carl', 3),\n",
       " ('nerv', 3),\n",
       " ('biospher', 3),\n",
       " ('madagascan', 3),\n",
       " ('emit', 3),\n",
       " ('confirm', 3),\n",
       " ('rise', 3),\n",
       " ('adjuv', 3),\n",
       " ('unclean', 3),\n",
       " ('tsk', 3),\n",
       " ('him', 2),\n",
       " ('trial', 2),\n",
       " ('they', 2),\n",
       " ('from', 2),\n",
       " ('it', 2),\n",
       " ('s', 2),\n",
       " ('differ', 2),\n",
       " ('caus', 2),\n",
       " ('prof', 2),\n",
       " ('drug', 2),\n",
       " ('sister', 2),\n",
       " ('conduct', 2),\n",
       " ('oxford', 2),\n",
       " ('by', 2),\n",
       " ('freedom', 2),\n",
       " ('choic', 2),\n",
       " ('imbal', 2),\n",
       " ('make', 2),\n",
       " ('creator', 2),\n",
       " ('ha', 2),\n",
       " ('die', 2),\n",
       " ('suffer', 2),\n",
       " ('etc', 2),\n",
       " ('perspect', 2),\n",
       " ('compani', 2),\n",
       " ('mani', 2),\n",
       " ('global', 2),\n",
       " ('matter', 2),\n",
       " ('type', 2),\n",
       " ('famili', 2),\n",
       " ('coolaid', 2),\n",
       " ('black', 2),\n",
       " ('immun', 2),\n",
       " ('advoc', 2),\n",
       " ('bill', 2),\n",
       " ('argument', 2),\n",
       " ('mention', 2),\n",
       " ('option', 2),\n",
       " ('littl', 2),\n",
       " ('johnson', 2),\n",
       " ('off', 2),\n",
       " ('n', 2),\n",
       " ('toy', 2),\n",
       " ('social', 2),\n",
       " ('charg', 2),\n",
       " ('list', 2),\n",
       " ('yall', 2),\n",
       " ('mask', 2),\n",
       " ('per', 2),\n",
       " ('chimp', 2),\n",
       " ('bot', 2),\n",
       " ('measl', 2),\n",
       " ('statu', 2),\n",
       " ('herd', 2),\n",
       " ('economi', 2),\n",
       " ('child', 2),\n",
       " ('christian', 2),\n",
       " ('bc', 2),\n",
       " ('she', 2),\n",
       " ('import', 2),\n",
       " ('nono', 2),\n",
       " ('immedi', 2),\n",
       " ('technolog', 2),\n",
       " ('bought', 2),\n",
       " ('yk', 2),\n",
       " ('zuma', 2),\n",
       " ('scam', 2),\n",
       " ('altern', 2),\n",
       " ('increas', 2),\n",
       " ('tb', 2),\n",
       " ('law', 2),\n",
       " ('meh', 2),\n",
       " ('99', 2),\n",
       " ('lethal', 2),\n",
       " ('enslav', 2),\n",
       " ('decreas', 2),\n",
       " ('adulthood', 2),\n",
       " ('convinc', 2),\n",
       " ('gimmick', 2),\n",
       " ('flaw', 2),\n",
       " ('libya', 2),\n",
       " ('standard', 2),\n",
       " ('anim', 2),\n",
       " ('wef', 2),\n",
       " ('confer', 2),\n",
       " ('highli', 2),\n",
       " ('featur', 2),\n",
       " ('tower', 2),\n",
       " ('respect', 2),\n",
       " ('vector', 2),\n",
       " ('fishi', 2),\n",
       " ('59', 2),\n",
       " ('cancer', 2),\n",
       " ('finish', 2),\n",
       " ('dismiss', 2),\n",
       " ('insan', 2),\n",
       " ('function', 2),\n",
       " ('mankind', 2),\n",
       " ('militari', 2),\n",
       " ('vanuatu', 2),\n",
       " ('australian', 2),\n",
       " ('employe', 2),\n",
       " ('involv', 2),\n",
       " ('revis', 2),\n",
       " ('downward', 2),\n",
       " ('deceit', 2),\n",
       " ('forev', 2),\n",
       " ('weed', 2),\n",
       " ('perfect', 2),\n",
       " ('initi', 2),\n",
       " ('ayo', 2),\n",
       " ('azt', 2),\n",
       " ('emb', 2),\n",
       " ('bibl', 2),\n",
       " ('herb', 2),\n",
       " ('chart', 2),\n",
       " ('assess', 2),\n",
       " ('jackson', 2),\n",
       " ('null', 2),\n",
       " ('financi', 2),\n",
       " ('thsi', 2),\n",
       " ('occur', 2),\n",
       " ('bow', 2),\n",
       " ('dat', 2),\n",
       " ('paul', 2),\n",
       " ('karin', 2),\n",
       " ('q', 2),\n",
       " ('vaxx', 2),\n",
       " ('thru', 2),\n",
       " ('tonsil', 2),\n",
       " ('elect', 2),\n",
       " ('known', 2),\n",
       " ('org', 2),\n",
       " ('wealthi', 2),\n",
       " ('cognit', 2),\n",
       " ('basher', 2),\n",
       " ('idc', 2),\n",
       " ('watt', 2),\n",
       " ('misconcept', 2),\n",
       " ('shade', 2),\n",
       " ('scoff', 2),\n",
       " ('whichev', 2),\n",
       " ('passport', 2),\n",
       " ('largest', 2),\n",
       " ('superior', 2),\n",
       " ('unregul', 2),\n",
       " ('lest', 2),\n",
       " ('mikovitz', 2),\n",
       " ('written', 2),\n",
       " ('boost', 2),\n",
       " ('blog', 2),\n",
       " ('madagasca', 2),\n",
       " ('hay', 2),\n",
       " ('cobalt', 2),\n",
       " ('b12', 2),\n",
       " ('viewpoint', 2),\n",
       " ('metabol', 2),\n",
       " ('valuabl', 2),\n",
       " ('hurri', 2),\n",
       " ('sotho', 2),\n",
       " ('inflammatori', 2),\n",
       " ('scholar', 2),\n",
       " ('sheak', 2),\n",
       " ('quo', 2),\n",
       " ('incom', 2),\n",
       " ('hong', 2),\n",
       " ('kong', 2),\n",
       " ('densiti', 2),\n",
       " ('avg', 2),\n",
       " ('squar', 2),\n",
       " ('km', 2),\n",
       " ('slow', 2),\n",
       " ('alzheim', 2),\n",
       " ('doc', 2),\n",
       " ('refug', 2),\n",
       " ('penalti', 2),\n",
       " ('anecdot', 2),\n",
       " ('falsifi', 2),\n",
       " ('unfalsifi', 2),\n",
       " ('teapot', 2),\n",
       " ('scar', 2),\n",
       " ('perfectli', 2),\n",
       " ('rebel', 2),\n",
       " ('ramaphosa', 1),\n",
       " ('is', 1),\n",
       " ('not', 1),\n",
       " ('a', 1),\n",
       " ('stupid', 1),\n",
       " ('he', 1),\n",
       " ('whst', 1),\n",
       " ('best', 1),\n",
       " ('for', 1),\n",
       " ('the', 1),\n",
       " ('same', 1),\n",
       " ('take', 1),\n",
       " ('place', 1),\n",
       " ('in', 1),\n",
       " ('uk', 1),\n",
       " ('and', 1),\n",
       " ('us', 1),\n",
       " ('you', 1),\n",
       " ('re', 1),\n",
       " ('talk', 1),\n",
       " ('rubbish', 1),\n",
       " ('dian', 1),\n",
       " ('wieseat', 1),\n",
       " ('least', 1),\n",
       " ('i', 1),\n",
       " ('am', 1),\n",
       " ('garbag', 1),\n",
       " ('becaus', 1),\n",
       " ('cannot', 1),\n",
       " ('say', 1),\n",
       " ('certainti', 1),\n",
       " ('what', 1),\n",
       " ('are', 1),\n",
       " ('inject', 1),\n",
       " ('to', 1),\n",
       " ('those', 1),\n",
       " ('south', 1),\n",
       " ('african', 1),\n",
       " ('wies', 1),\n",
       " ('certain', 1),\n",
       " ('of', 1),\n",
       " ('vaccin', 1),\n",
       " ('be', 1),\n",
       " ('qualiti', 1),\n",
       " ('quantiti', 1),\n",
       " ('concentr', 1),\n",
       " ('product', 1),\n",
       " ('materi', 1),\n",
       " ('mroots1', 1),\n",
       " ('how', 1),\n",
       " ('hell', 1),\n",
       " ('all', 1),\n",
       " ('voluntari', 1),\n",
       " ('no', 1),\n",
       " ('one', 1),\n",
       " ('forc', 1),\n",
       " ('or', 1),\n",
       " ('coerc', 1),\n",
       " ('into', 1),\n",
       " ('volunt', 1),\n",
       " ('some', 1),\n",
       " ('peopl', 1),\n",
       " ('will', 1),\n",
       " ('out', 1),\n",
       " ('good', 1),\n",
       " ('their', 1),\n",
       " ('heart', 1),\n",
       " ('trust', 1),\n",
       " ('trail', 1),\n",
       " ('oversea', 1),\n",
       " ('kind', 1),\n",
       " ('problem', 1),\n",
       " ('down', 1),\n",
       " ('road', 1),\n",
       " ('agre', 1),\n",
       " ('start', 1),\n",
       " ('with', 1),\n",
       " ('maadi', 1),\n",
       " ('hi', 1),\n",
       " ('bra', 1),\n",
       " ('abdroool', 1),\n",
       " ('gangster', 1),\n",
       " ('greedi', 1),\n",
       " ('knive', 1),\n",
       " ('crook', 1),\n",
       " ('have', 1),\n",
       " ('them', 1),\n",
       " ('do', 1),\n",
       " ('if', 1),\n",
       " ('that', 1),\n",
       " ('but', 1),\n",
       " ('case', 1),\n",
       " ('test', 1),\n",
       " ('did', 1),\n",
       " ('we', 1),\n",
       " ('get', 1),\n",
       " ('med', 1),\n",
       " ('world', 1),\n",
       " ('selfless', 1),\n",
       " ('realiti', 1),\n",
       " ('somali', 1),\n",
       " ('right', 1),\n",
       " ('congratul', 1),\n",
       " ('africa', 1),\n",
       " ('especi', 1),\n",
       " ('first', 1),\n",
       " ('covid', 1),\n",
       " ('19', 1),\n",
       " ('vax', 1),\n",
       " ('although', 1),\n",
       " ('prefer', 1),\n",
       " ('independ', 1),\n",
       " ('local', 1),\n",
       " ('run', 1),\n",
       " ('research', 1),\n",
       " ('nevertheless', 1),\n",
       " ('ok', 1),\n",
       " ('should', 1),\n",
       " ('thi', 1),\n",
       " ('stay', 1),\n",
       " ('long', 1),\n",
       " ('haul', 1),\n",
       " ('even', 1),\n",
       " ('most', 1),\n",
       " ('who', 1),\n",
       " ('recov', 1),\n",
       " ('last', 1),\n",
       " ('physic', 1),\n",
       " ('psycholog', 1),\n",
       " ('damag', 1),\n",
       " ('so', 1),\n",
       " ('prevent', 1),\n",
       " ('way', 1),\n",
       " ('better', 1),\n",
       " ('than', 1),\n",
       " ('cure', 1),\n",
       " ('let', 1),\n",
       " ('alon', 1),\n",
       " ('ignor', 1),\n",
       " ('conspiraci', 1),\n",
       " ('theori', 1),\n",
       " ('about', 1),\n",
       " ('bullshit', 1),\n",
       " ('ani', 1),\n",
       " ('doctor', 1),\n",
       " ('accept', 1),\n",
       " ('http', 1),\n",
       " ('youtu', 1),\n",
       " ('ftqhepw', 1),\n",
       " ('rg', 1),\n",
       " ('look', 1),\n",
       " ('easteregg', 1),\n",
       " ('can', 1),\n",
       " ('find', 1),\n",
       " ('ban', 1),\n",
       " ('lung', 1),\n",
       " ('tissu', 1),\n",
       " ('video', 1),\n",
       " ('straight', 1),\n",
       " ('fund', 1),\n",
       " ('gate', 1),\n",
       " ('foundat', 1),\n",
       " ('use', 1),\n",
       " ('famou', 1),\n",
       " ('as', 1),\n",
       " ('whi', 1),\n",
       " ('pandem', 1),\n",
       " ('real', 1),\n",
       " ('liter', 1),\n",
       " ('nobodi', 1),\n",
       " ('anyon', 1),\n",
       " ('against', 1),\n",
       " ('brother', 1),\n",
       " ('on', 1),\n",
       " ('themselv', 1),\n",
       " ('theyr', 1),\n",
       " ('nanotechnolog', 1),\n",
       " ('alter', 1),\n",
       " ('our', 1),\n",
       " ('genet', 1),\n",
       " ('steveninth', 1),\n",
       " ('search', 1),\n",
       " ('thank', 1),\n",
       " ('info', 1),\n",
       " ('up', 1),\n",
       " ('point', 1),\n",
       " ('except', 1),\n",
       " ('where', 1),\n",
       " ('there', 1),\n",
       " ('an', 1),\n",
       " ('natur', 1),\n",
       " ('never', 1),\n",
       " ('mistak', 1),\n",
       " ('which', 1),\n",
       " ('mean', 1),\n",
       " ('caucian', 1),\n",
       " ('white', 1),\n",
       " ('expir', 1),\n",
       " ('time', 1),\n",
       " ('planet', 1),\n",
       " ('slowli', 1),\n",
       " ('reduc', 1),\n",
       " ('govern', 1),\n",
       " ('been', 1),\n",
       " ('work', 1),\n",
       " ('overtim', 1),\n",
       " ('encourag', 1),\n",
       " ('women', 1),\n",
       " ('more', 1),\n",
       " ('children', 1),\n",
       " ('help', 1),\n",
       " ('fertil', 1),\n",
       " ('treatment', 1),\n",
       " ('fact', 1),\n",
       " ('pay', 1),\n",
       " ('live', 1),\n",
       " ('europ', 1),\n",
       " ('go', 1),\n",
       " ('manqoba', 1),\n",
       " ('mkhwanazi', 1),\n",
       " ('control', 1),\n",
       " ('media', 1),\n",
       " ('strike', 1),\n",
       " ('fear', 1),\n",
       " ('corona', 1),\n",
       " ('viru', 1),\n",
       " ('fake', 1),\n",
       " ('enforc', 1),\n",
       " ('coronaviru', 1),\n",
       " ('isnt', 1),\n",
       " ('mylifematt', 1),\n",
       " ('vuj6qg7oxhw', 1),\n",
       " ('watch', 1),\n",
       " ('these', 1),\n",
       " ('channel', 1),\n",
       " ('dreami', 1),\n",
       " ('mad', 1),\n",
       " ('wtf', 1),\n",
       " ('dude', 1),\n",
       " ('u', 1),\n",
       " ('serious', 1),\n",
       " ('uneduc', 1),\n",
       " ('nurs', 1),\n",
       " ('usa', 1),\n",
       " ('cri', 1),\n",
       " ('whom', 1),\n",
       " ('got', 1),\n",
       " ('here', 1),\n",
       " ('actor', 1),\n",
       " ('too', 1),\n",
       " ('employ', 1),\n",
       " ('entir', 1),\n",
       " ('act', 1),\n",
       " ('movi', 1),\n",
       " ('rf', 1),\n",
       " ('chip', 1),\n",
       " ('fuck', 1),\n",
       " ('sake', 1),\n",
       " ('think', 1),\n",
       " ('want', 1),\n",
       " ('tomonitor', 1),\n",
       " ('rashid', 1),\n",
       " ('shit', 1),\n",
       " ('whole', 1),\n",
       " ('new', 1),\n",
       " ('level', 1),\n",
       " ('put', 1),\n",
       " ('thing', 1),\n",
       " ('bodi', 1),\n",
       " ('made', 1),\n",
       " ('million', 1),\n",
       " ('upon', 1),\n",
       " ('virus', 1),\n",
       " ('harm', 1),\n",
       " ('scientif', 1),\n",
       " ('model', 1),\n",
       " ('germ', 1),\n",
       " ('hoax', 1),\n",
       " ('power', 1),\n",
       " ('pharma', 1),\n",
       " ('own', 1),\n",
       " ('process', 1),\n",
       " ('mass', 1),\n",
       " ('now', 1),\n",
       " ('abl', 1),\n",
       " ('mafia', 1),\n",
       " ('conglomer', 1),\n",
       " ('popul', 1),\n",
       " ('much', 1),\n",
       " ('logic', 1),\n",
       " ('goe', 1),\n",
       " ('window', 1),\n",
       " ('devil', 1),\n",
       " ('accord', 1),\n",
       " ('websit', 1),\n",
       " ('consid', 1),\n",
       " ('risk', 1),\n",
       " ('contag', 1),\n",
       " ('lie', 1),\n",
       " ('afrika', 1),\n",
       " ('alreadi', 1),\n",
       " ('itself', 1),\n",
       " ('without', 1),\n",
       " ('realis', 1),\n",
       " ('fantast', 1),\n",
       " ('machin', 1),\n",
       " ('psychopath', 1),\n",
       " ('believ', 1),\n",
       " ('surviv', 1),\n",
       " ('ancestor', 1),\n",
       " ('year', 1),\n",
       " ('adapt', 1),\n",
       " ('climat', 1),\n",
       " ('inherit', 1),\n",
       " ('dna', 1),\n",
       " ('deal', 1),\n",
       " ('certainli', 1),\n",
       " ('enemi', 1),\n",
       " ('idiot', 1),\n",
       " ('continu', 1),\n",
       " ('play', 1),\n",
       " ('mind', 1),\n",
       " ('game', 1),\n",
       " ('surpris', 1),\n",
       " ('shiva', 1),\n",
       " ('buttar', 1),\n",
       " ('kaufmann', 1),\n",
       " ('kumaran', 1),\n",
       " ('govend', 1),\n",
       " ('correct', 1),\n",
       " ('yo', 1),\n",
       " ('care', 1),\n",
       " ('appreci', 1),\n",
       " ('bravoooooooooooooo', 1),\n",
       " ('smart', 1),\n",
       " ('onli', 1),\n",
       " ('fight', 1),\n",
       " ('evil', 1),\n",
       " ('sometim', 1),\n",
       " ('hate', 1),\n",
       " ('violenc', 1),\n",
       " ('self', 1),\n",
       " ('defens', 1),\n",
       " ('me', 1),\n",
       " ('readi', 1),\n",
       " ('face', 1),\n",
       " ('demon', 1),\n",
       " ('worker', 1),\n",
       " ('wrath', 1),\n",
       " ('man', 1),\n",
       " ('doe', 1),\n",
       " ('produc', 1),\n",
       " ('righteous', 1),\n",
       " ('god', 1),\n",
       " ('away', 1),\n",
       " ('gun', 1),\n",
       " ('dankiee', 1),\n",
       " ('malema', 1),\n",
       " ('warn', 1),\n",
       " ('cyril', 1),\n",
       " ('love', 1),\n",
       " ('money', 1),\n",
       " ('over', 1),\n",
       " ('see', 1),\n",
       " ('edgar', 1),\n",
       " ('papa', 1),\n",
       " ('maduna', 1),\n",
       " ('just', 1),\n",
       " ('sit', 1),\n",
       " ('back', 1),\n",
       " ('yu', 1),\n",
       " ('histori', 1),\n",
       " ('understand', 1),\n",
       " ('regul', 1),\n",
       " ('western', 1),\n",
       " ('uneth', 1),\n",
       " ('practic', 1),\n",
       " ('clearli', 1),\n",
       " ('develop', 1),\n",
       " ('countri', 1),\n",
       " ('like', 1),\n",
       " ('were', 1),\n",
       " ('lace', 1),\n",
       " ('hhcg', 1),\n",
       " ('signific', 1),\n",
       " ('number', 1),\n",
       " ('ever', 1),\n",
       " ('ye', 1),\n",
       " ('other', 1),\n",
       " ('insidi', 1),\n",
       " ('carri', 1),\n",
       " ('across', 1),\n",
       " ('don', 1),\n",
       " ('t', 1),\n",
       " ('need', 1),\n",
       " ('hard', 1),\n",
       " ('inform', 1),\n",
       " ('stop', 1),\n",
       " ('drink', 1),\n",
       " ('quit', 1),\n",
       " ('imagin', 1),\n",
       " ('racism', 1),\n",
       " ('everi', 1),\n",
       " ('event', 1),\n",
       " ('agenda', 1),\n",
       " ('west', 1),\n",
       " ('killer', 1),\n",
       " ('arriv', 1),\n",
       " ('africawhi', 1),\n",
       " ('unbeliev', 1),\n",
       " ('ass', 1),\n",
       " ('check', 1),\n",
       " ('your', 1),\n",
       " ('post', 1),\n",
       " ('despic', 1),\n",
       " ('sheepl', 1),\n",
       " ('lead', 1),\n",
       " ('slaughter', 1),\n",
       " ('through', 1),\n",
       " ('life', 1),\n",
       " ('scare', 1),\n",
       " ('label', 1),\n",
       " ('probabl', 1),\n",
       " ('couldn', 1),\n",
       " ('less', 1),\n",
       " ('scienc', 1),\n",
       " ('public', 1),\n",
       " ('refer', 1),\n",
       " ('person', 1),\n",
       " ('call', 1),\n",
       " ('jesu', 1),\n",
       " ('doesn', 1),\n",
       " ('walk', 1),\n",
       " ('water', 1),\n",
       " ('nor', 1),\n",
       " ('name', 1),\n",
       " ('dusti', 1),\n",
       " ('old', 1),\n",
       " ('book', 1),\n",
       " ('left', 1),\n",
       " ('rule', 1),\n",
       " ('unfortun', 1),\n",
       " ('ve', 1),\n",
       " ('blame', 1),\n",
       " ('when', 1),\n",
       " ('kick', 1),\n",
       " ('institut', 1),\n",
       " ('repent', 1),\n",
       " ('invit', 1),\n",
       " ('nation', 1),\n",
       " ('rid', 1),\n",
       " ('allow', 1),\n",
       " ('still', 1),\n",
       " ('k', 1),\n",
       " ('lhen', 1),\n",
       " ('went', 1),\n",
       " ('acknowledg', 1),\n",
       " ('seek', 1),\n",
       " ('ancient', 1),\n",
       " ('spiritu', 1),\n",
       " ('instead', 1),\n",
       " ('fed', 1),\n",
       " ('ounc', 1),\n",
       " ('human', 1),\n",
       " ('creat', 1),\n",
       " ('destroy', 1),\n",
       " ('indigen', 1),\n",
       " ('claim', 1),\n",
       " ('testament', 1),\n",
       " ('commit', 1),\n",
       " ('hintw', 1),\n",
       " ('sold', 1),\n",
       " ('odwa', 1),\n",
       " ('ngqavu', 1),\n",
       " ('order', 1),\n",
       " ('someth', 1),\n",
       " ('adrea', 1),\n",
       " ('ambassador', 1),\n",
       " ('carrol', 1),\n",
       " ('her', 1),\n",
       " ('everyon', 1),\n",
       " ('then', 1),\n",
       " ('infect', 1),\n",
       " ('ebola', 1),\n",
       " ('similar', 1),\n",
       " ('flu', 1),\n",
       " ('threat', 1),\n",
       " ('isn', 1),\n",
       " ('realli', 1),\n",
       " ('anoth', 1),\n",
       " ('sheep', 1),\n",
       " ('eye', 1),\n",
       " ('wake', 1),\n",
       " ('china', 1),\n",
       " ('america', 1),\n",
       " ('well', 1),\n",
       " ('sure', 1),\n",
       " ('doubt', 1),\n",
       " ('predomin', 1),\n",
       " ('cool', 1),\n",
       " ('aid', 1),\n",
       " ('steve', 1),\n",
       " ('floit', 1),\n",
       " ('bet', 1),\n",
       " ('wouldn', 1),\n",
       " ('lol', 1),\n",
       " ('seen', 1),\n",
       " ('forget', 1),\n",
       " ('scientist', 1),\n",
       " ('done', 1),\n",
       " ('past', 1),\n",
       " ('tuskege', 1),\n",
       " ('peru', 1),\n",
       " ('central', 1),\n",
       " ('victim', 1),\n",
       " ('fals', 1),\n",
       " ('hope', 1),\n",
       " ('syphilli', 1),\n",
       " ('silvest', 1),\n",
       " ('hercul', 1),\n",
       " ('strawman', 1),\n",
       " ('happili', 1),\n",
       " ('happen', 1),\n",
       " ('brazil', 1),\n",
       " ('american', 1),\n",
       " ('half', 1),\n",
       " ('centuri', 1),\n",
       " ('ago', 1),\n",
       " ('educ', 1),\n",
       " ('step', 1),\n",
       " ('today', 1),\n",
       " ('wish', 1),\n",
       " ('particip', 1),\n",
       " ('fine', 1),\n",
       " ('absurd', 1),\n",
       " ('numer', 1),\n",
       " ('origin', 1),\n",
       " ('wa', 1),\n",
       " ('distort', 1),\n",
       " ('everyth', 1),\n",
       " ('took', 1),\n",
       " ('huge', 1),\n",
       " ('toward', 1),\n",
       " ('misinform', 1),\n",
       " ('attempt', 1),\n",
       " ('swallow', 1),\n",
       " ('decept', 1),\n",
       " ('someday', 1),\n",
       " ('predominantli', 1),\n",
       " ('each', 1),\n",
       " ('lot', 1),\n",
       " ('faith', 1),\n",
       " ('luck', 1),\n",
       " ('also', 1),\n",
       " ('common', 1),\n",
       " ('sens', 1),\n",
       " ('choos', 1),\n",
       " ('benefit', 1),\n",
       " ('catharin', 1),\n",
       " ('afford', 1),\n",
       " ('question', 1),\n",
       " ('econom', 1),\n",
       " ('veri', 1),\n",
       " ('oh', 1),\n",
       " ('meant', 1),\n",
       " ('context', 1),\n",
       " ('hasn', 1),\n",
       " ('month', 1),\n",
       " ('m', 1),\n",
       " ('won', 1),\n",
       " ('discov', 1),\n",
       " ('onc', 1),\n",
       " ('give', 1),\n",
       " ('odd', 1),\n",
       " ('inde', 1),\n",
       " ('small', 1),\n",
       " ('chose', 1),\n",
       " ('cheap', 1),\n",
       " ('easi', 1),\n",
       " ('koolaid', 1),\n",
       " ('big', 1),\n",
       " ('comment', 1),\n",
       " ('jimmi', 1),\n",
       " ('carr', 1),\n",
       " ('fan', 1),\n",
       " ('els', 1),\n",
       " ('steril', 1),\n",
       " ('thousand', 1),\n",
       " ('woman', 1),\n",
       " ('india', 1),\n",
       " ('come', 1),\n",
       " ('poor', 1),\n",
       " ('illus', 1),\n",
       " ('lab', 1),\n",
       " ('rat', 1),\n",
       " ('explain', 1),\n",
       " ('50', 1),\n",
       " ('60', 1),\n",
       " ('purpos', 1),\n",
       " ('around', 1),\n",
       " ('commun', 1),\n",
       " ('follow', 1),\n",
       " ('iowa', 1),\n",
       " ('girl', 1),\n",
       " ('amp', 1),\n",
       " ('glen', 1),\n",
       " ('pharmaceut', 1),\n",
       " ('court', 1),\n",
       " ('stolen', 1),\n",
       " ('bio', 1),\n",
       " ('patient', 1),\n",
       " ('befor', 1),\n",
       " ('covid19', 1),\n",
       " ('far', 1),\n",
       " ('illeg', 1),\n",
       " ('haven', 1),\n",
       " ('attent', 1),\n",
       " ('organ', 1),\n",
       " ('sell', 1),\n",
       " ('presid', 1),\n",
       " ('snow', 1),\n",
       " ('grass', 1),\n",
       " ('news', 1),\n",
       " ('smh', 1),\n",
       " ('blk', 1),\n",
       " ('ain', 1),\n",
       " ('fking', 1),\n",
       " ('guy', 1),\n",
       " ('ona', 1),\n",
       " ('fkup', 1),\n",
       " ('smelli', 1),\n",
       " ('hand', 1),\n",
       " ('everywher', 1),\n",
       " ('spineless', 1),\n",
       " ('doesnt', 1),\n",
       " ('gut', 1),\n",
       " ('must', 1),\n",
       " ('rememb', 1),\n",
       " ('wast', 1),\n",
       " ('prepar', 1),\n",
       " ('refus', 1),\n",
       " ('exactli', 1),\n",
       " ('merci', 1),\n",
       " ('grace', 1),\n",
       " ('mg', 1),\n",
       " ('great', 1),\n",
       " ('servic', 1),\n",
       " ('content', 1),\n",
       " ('madagascar', 1),\n",
       " ('possibl', 1),\n",
       " ('read', 1),\n",
       " ('stole', 1),\n",
       " ('hotel', 1),\n",
       " ('late', 1),\n",
       " ('at', 1),\n",
       " ('head', 1),\n",
       " ('line', 1),\n",
       " ('flow', 1),\n",
       " ('journey', 1),\n",
       " ('higher', 1),\n",
       " ('80', 1),\n",
       " ('percent', 1),\n",
       " ('mortal', 1),\n",
       " ('afraid', 1),\n",
       " ('favor', 1),\n",
       " ('bud', 1),\n",
       " ('open', 1),\n",
       " ('googl', 1),\n",
       " ('browser', 1),\n",
       " ('four', 1),\n",
       " ('digit', 1),\n",
       " ('three', 1),\n",
       " ('may', 1),\n",
       " ('after', 1),\n",
       " ('mate', 1),\n",
       " ('henri', 1),\n",
       " ('david', 1),\n",
       " ('gonna', 1),\n",
       " ('donkey', 1),\n",
       " ('vote', 1),\n",
       " ('listen', 1),\n",
       " ('remind', 1),\n",
       " ('june', 1),\n",
       " ('16', 1),\n",
       " ('1976', 1),\n",
       " ('busi', 1),\n",
       " ('oper', 1),\n",
       " ('soweto', 1),\n",
       " ('kenin', 1),\n",
       " ('stuurman', 1),\n",
       " ('seem', 1),\n",
       " ('truli', 1),\n",
       " ('safeti', 1),\n",
       " ('industri', 1),\n",
       " ('cold', 1),\n",
       " ('200', 1),\n",
       " ('rhinovirus', 1),\n",
       " ('coronavirus', 1),\n",
       " ('magic', 1),\n",
       " ('plane', 1),\n",
       " ('fli', 1),\n",
       " ('sar', 1),\n",
       " ('vaccinem', 1),\n",
       " ('vaccineit', 1),\n",
       " ('15', 1),\n",
       " ('yearshow', 1),\n",
       " ('becom', 1),\n",
       " ('avail', 1),\n",
       " ('18', 1),\n",
       " ('experi', 1),\n",
       " ('yep', 1),\n",
       " ('sad', 1),\n",
       " ('dj', 1),\n",
       " ('papzin', 1),\n",
       " ('glad', 1),\n",
       " ('medic', 1),\n",
       " ('didn', 1),\n",
       " ('attitud', 1),\n",
       " ('polio', 1),\n",
       " ('ravag', 1),\n",
       " ('futur', 1),\n",
       " ('commentari', 1),\n",
       " ('mutat', 1),\n",
       " ('rapidli', 1),\n",
       " ('useless', 1),\n",
       " ('neema', 1),\n",
       " ('kyamba', 1),\n",
       " ('hmm', 1),\n",
       " ('chang', 1),\n",
       " ('base', 1),\n",
       " ('unknown', 1),\n",
       " ('youtub', 1),\n",
       " ('such', 1),\n",
       " ('dilemma', 1),\n",
       " ('plan', 1),\n",
       " ('enzym', 1),\n",
       " ('resourc', 1),\n",
       " ('pleas', 1),\n",
       " ('advis', 1),\n",
       " ('9zqr4bvarj0', 1),\n",
       " ('vmat2', 1),\n",
       " ('gene', 1),\n",
       " ('fool', 1),\n",
       " ('yourself', 1),\n",
       " ('hahaha', 1),\n",
       " ('reach', 1),\n",
       " ('valid', 1),\n",
       " ('fail', 1),\n",
       " ('miser', 1),\n",
       " ('low', 1),\n",
       " ('compar', 1),\n",
       " ('period', 1),\n",
       " ('given', 1),\n",
       " ('race', 1),\n",
       " ('stoop', 1),\n",
       " ('factual', 1),\n",
       " ('piss', 1),\n",
       " ('shame', 1),\n",
       " ('earth', 1),\n",
       " ('flat', 1),\n",
       " ('gtfo', 1),\n",
       " ('centr', 1),\n",
       " ('develpp', 1),\n",
       " ('absolut', 1),\n",
       " ('truth', 1),\n",
       " ('ed', 1),\n",
       " ('g', 1),\n",
       " ('tell', 1),\n",
       " ('said', 1),\n",
       " ('death', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_word_counts_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Arranging in a descending order\n",
    "reply_word_counts_sorted = sorted(replies_word_counts.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('description', 1),\n",
       " ('irrelevant', 1),\n",
       " ('formal', 1),\n",
       " ('poliomyelitis', 1),\n",
       " ('iron', 1),\n",
       " ('validate', 1),\n",
       " ('spare', 1),\n",
       " ('patch', 1),\n",
       " ('arrogant', 1),\n",
       " ('pastor', 1),\n",
       " ('vulgar', 1),\n",
       " ('guest', 1),\n",
       " ('cheek', 1),\n",
       " ('drank', 1),\n",
       " ('jess', 1),\n",
       " ('takeover', 1),\n",
       " ('smokescreen', 1),\n",
       " ('squeeze', 1),\n",
       " ('bag', 1),\n",
       " ('harsh', 1)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_word_counts_sorted[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Word Embeddings using gensim an NLP library\n",
    "embedding  = word2vec.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming the words\n",
    "ps = PorterStemmer() \n",
    "stemmed_reply_tokens = []\n",
    "for i in range(len(reply_tokens)):\n",
    "    sent = []\n",
    "    for j in range(len(reply_tokens[i])):\n",
    "        sent.append(ps.stem(reply_tokens[i][j]))\n",
    "    stemmed_reply_tokens.append(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
